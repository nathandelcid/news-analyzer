{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_data\n",
    "from features import *\n",
    "\n",
    "df = preprocess_data(\"data/5-min-all.csv\")#.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2436f0",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering (Baseline Regression/Classification Models)\n",
    "\n",
    "## $\\text{SAM}_{12}$, $\\text{SAM}_{26}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532e0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_sma(df, column=\"CLOSE\", window=12)\n",
    "df = add_sma(df, column=\"CLOSE\", window=26)\n",
    "\n",
    "df = df.drop_nulls(subset=[\"SMA_12\", \"SMA_26\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cf56b",
   "metadata": {},
   "source": [
    "## $\\text{EMA}_{12}$, $\\text{EMA}_{26}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f678e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_ema(df, column=\"CLOSE\", window=12)\n",
    "df = add_ema(df, column=\"CLOSE\", window=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683138df",
   "metadata": {},
   "source": [
    "## $\\text{MACD}$ (MACD + Signal + Hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b540f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_macd(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a84164",
   "metadata": {},
   "source": [
    "## $\\text{Bollinger Bands}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad47e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_bollinger_bands(df, price_col=\"CLOSE\", window=20, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200602e6",
   "metadata": {},
   "source": [
    "## $\\text{RSI}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9393f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_rsi(df, price_col=\"CLOSE\", window=14)\n",
    "\n",
    "df.write_parquet(\"data/5min-features-clean.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9f251",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <span style=\"color: red; font-size: 48px; font-weight: bold;\">\n",
    "    IMPORTANT: RESTART KERNEL HERE\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce6062",
   "metadata": {},
   "source": [
    "# 3. Scale / Normalize (Per-Ticker Z-Score)\n",
    "\n",
    "## Drop `null` columns as well as other columns we no longer need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340ac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_model = pl.read_parquet(\"data/5min-features-clean.parquet\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"CLOSE\",\n",
    "    \"VOL\",\n",
    "    \"SMA_12\", \"SMA_26\",\n",
    "    \"EMA_12\", \"EMA_26\",\n",
    "    \"MACD\", \"MACD_SIGNAL\", \"MACD_HIST\",\n",
    "    \"BB_UPPER_20\", \"BB_LOWER_20\", \"RSI_14\",\n",
    "]\n",
    "\n",
    "meta_cols = [\"EXCHANGE\", \"TICKER\", \"TIMESTAMP\"]\n",
    "target_col = \"UP_NEXT\"\n",
    "\n",
    "cols_to_keep = meta_cols + feature_cols + [target_col]\n",
    "df_model = df_model.select([c for c in df_model.columns if c in cols_to_keep])\n",
    "\n",
    "df_model = df_model.drop_nulls(subset=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855a344",
   "metadata": {},
   "source": [
    "## Normalize features per ticker to make high-value stocks (e.g., AAPL) comparable with lower-value stocks (e.g., $3 stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed755da",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"CLOSE\",\n",
    "    \"VOL\",\n",
    "    \"SMA_12\", \"SMA_26\",\n",
    "    \"EMA_12\", \"EMA_26\",\n",
    "    \"MACD\", \"MACD_SIGNAL\", \"MACD_HIST\",\n",
    "    \"BB_UPPER_20\", \"BB_LOWER_20\", \"RSI_14\",\n",
    "]\n",
    "\n",
    "for col in feature_cols:\n",
    "    df_model = df_model.with_columns(\n",
    "        (\n",
    "            (pl.col(col) - pl.col(col).mean().over(\"TICKER\"))\n",
    "            / pl.col(col).std().over(\"TICKER\")\n",
    "        ).alias(f\"{col}_Z\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e24ab",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <span style=\"color: red; font-size: 48px; font-weight: bold;\">\n",
    "    IMPORTANT: RESTART KERNEL HERE\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa2db5",
   "metadata": {},
   "source": [
    "## Z-Score Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895533a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "z_cols = [f\"{c}_Z\" for c in feature_cols]\n",
    "df_model = df_model.with_columns([\n",
    "    pl.col(c).clip(lower_bound=-5.0, upper_bound=5.0).alias(c) for c in z_cols\n",
    "])\n",
    "\n",
    "df_model.write_parquet(\"data/5min-features-model.parquet\")\n",
    "old_file = \"data/5min-features-clean.parquet\"\n",
    "os.remove(old_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3393ee",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fbc8a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <span style=\"color: red; font-size: 48px; font-weight: bold;\">\n",
    "    IMPORTANT: RESTART KERNEL HERE\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c37b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_model = pl.read_parquet(\"data/5min-features-model.parquet\")\n",
    "\n",
    "df_model.select(pl.col(\"UP_NEXT\").null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26db128",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(df_model, pl.LazyFrame):\n",
    "    df_collected = df_model.collect()\n",
    "else:\n",
    "    df_collected = df_model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATAFRAME OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nShape: {df_collected.shape[0]:,} rows Ã— {df_collected.shape[1]} columns\")\n",
    "print(f\"\\nMemory usage: {df_collected.estimated_size('mb'):.2f} MB\")\n",
    "print(f\"\\nColumns: {list(df_collected.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(df_collected.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST FEW ROWS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_collected.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LAST FEW ROWS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_collected.tail(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "missing = df_collected.null_count()\n",
    "print(missing)\n",
    "print(f\"\\nTotal missing values: {missing.sum_horizontal().item():,}\")\n",
    "print(f\"Percentage of missing values: {(missing.sum_horizontal().item() / (df_collected.shape[0] * df_collected.shape[1]) * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DUPLICATE ROWS\")\n",
    "print(\"=\" * 80)\n",
    "duplicates = df_collected.is_duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates:,}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates / df_collected.shape[0] * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY (NUMERICAL COLUMNS)\")\n",
    "print(\"=\" * 80)\n",
    "numerical_cols = [col for col, dtype in zip(df_collected.columns, df_collected.dtypes) \n",
    "                  if dtype in [pl.Int64, pl.Float64]]\n",
    "print(df_collected.select(numerical_cols).describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "categorical_cols = [col for col, dtype in zip(df_collected.columns, df_collected.dtypes) \n",
    "                    if dtype == pl.String]\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    value_counts = df_collected[col].value_counts().sort(\"count\", descending=True)\n",
    "    print(f\"  Unique values: {df_collected[col].n_unique()}\")\n",
    "    print(f\"  Top 10 values:\")\n",
    "    print(value_counts.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATE/TIME ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "if 'DATE' in df_collected.columns:\n",
    "    date_min = df_collected['DATE'].min()\n",
    "    date_max = df_collected['DATE'].max()\n",
    "    print(f\"DATE range: {date_min} to {date_max}\")\n",
    "    print(f\"Unique dates: {df_collected['DATE'].n_unique()}\")\n",
    "    \n",
    "if 'TIME' in df_collected.columns:\n",
    "    time_min = df_collected['TIME'].min()\n",
    "    time_max = df_collected['TIME'].max()\n",
    "    print(f\"TIME range: {time_min} to {time_max}\")\n",
    "    print(f\"Unique times: {df_collected['TIME'].n_unique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (IQR METHOD)\")\n",
    "print(\"=\" * 80)\n",
    "for col in numerical_cols:\n",
    "    if col in ['DATE', 'TIME', 'PER', 'OPENINT']:  # Skip ID-like columns\n",
    "        continue\n",
    "    q1 = df_collected[col].quantile(0.25)\n",
    "    q3 = df_collected[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = df_collected.filter(\n",
    "        (pl.col(col) < lower_bound) | (pl.col(col) > upper_bound)\n",
    "    ).shape[0]\n",
    "    print(f\"{col}: {outliers:,} outliers ({outliers/df_collected.shape[0]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZERO VALUES CHECK\")\n",
    "print(\"=\" * 80)\n",
    "for col in numerical_cols:\n",
    "    zeros = (df_collected[col] == 0).sum()\n",
    "    if zeros > 0:\n",
    "        print(f\"{col}: {zeros:,} zero values ({zeros/df_collected.shape[0]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEGATIVE VALUES CHECK\")\n",
    "print(\"=\" * 80)\n",
    "for col in numerical_cols:\n",
    "    if col in ['DATE', 'TIME', 'PER', 'OPENINT']:\n",
    "        continue\n",
    "    negatives = (df_collected[col] < 0).sum()\n",
    "    if negatives > 0:\n",
    "        print(f\"{col}: {negatives:,} negative values ({negatives/df_collected.shape[0]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION MATRIX (NUMERICAL FEATURES)\")\n",
    "print(\"=\" * 80)\n",
    "price_cols = [col for col in numerical_cols if col in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOL']]\n",
    "if len(price_cols) > 1:\n",
    "    # Compute correlation matrix using Polars native functions\n",
    "    corr_data = []\n",
    "    for col1 in price_cols:\n",
    "        row = []\n",
    "        for col2 in price_cols:\n",
    "            if col1 == col2:\n",
    "                row.append(1.0)\n",
    "            else:\n",
    "                corr_val = df_collected.select(\n",
    "                    pl.corr(col1, col2)\n",
    "                ).item()\n",
    "                row.append(round(corr_val, 3))\n",
    "        corr_data.append(row)\n",
    "    \n",
    "    # Print correlation matrix\n",
    "    print(f\"\\n{'':>10}\", end=\"\")\n",
    "    for col in price_cols:\n",
    "        print(f\"{col:>12}\", end=\"\")\n",
    "    print()\n",
    "    for i, col in enumerate(price_cols):\n",
    "        print(f\"{col:>10}\", end=\"\")\n",
    "        for val in corr_data[i]:\n",
    "            print(f\"{val:>12.3f}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "# Check for logical inconsistencies\n",
    "if all(col in df_collected.columns for col in ['HIGH', 'LOW', 'OPEN', 'CLOSE']):\n",
    "    invalid_high_low = df_collected.filter(pl.col('HIGH') < pl.col('LOW')).shape[0]\n",
    "    print(f\"Rows where HIGH < LOW: {invalid_high_low:,}\")\n",
    "    \n",
    "    invalid_open = df_collected.filter(\n",
    "        (pl.col('OPEN') > pl.col('HIGH')) | (pl.col('OPEN') < pl.col('LOW'))\n",
    "    ).shape[0]\n",
    "    print(f\"Rows where OPEN outside [LOW, HIGH]: {invalid_open:,}\")\n",
    "    \n",
    "    invalid_close = df_collected.filter(\n",
    "        (pl.col('CLOSE') > pl.col('HIGH')) | (pl.col('CLOSE') < pl.col('LOW'))\n",
    "    ).shape[0]\n",
    "    print(f\"Rows where CLOSE outside [LOW, HIGH]: {invalid_close:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE STATISTICS BY CATEGORY\")\n",
    "print(\"=\" * 80)\n",
    "if 'TICKER' in df_collected.columns:\n",
    "    print(\"\\nNumber of unique tickers:\", df_collected['TICKER'].n_unique())\n",
    "    print(\"\\nRows per ticker (top 10):\")\n",
    "    ticker_counts = df_collected.group_by('TICKER').agg(pl.len().alias('count')).sort('count', descending=True)\n",
    "    print(ticker_counts.head(10))\n",
    "\n",
    "if 'EXCHANGE' in df_collected.columns:\n",
    "    print(\"\\nNumber of unique exchanges:\", df_collected['EXCHANGE'].n_unique())\n",
    "    print(\"\\nRows per exchange:\")\n",
    "    exchange_counts = df_collected.group_by('EXCHANGE').agg(pl.len().alias('count')).sort('count', descending=True)\n",
    "    print(exchange_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a28292",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
